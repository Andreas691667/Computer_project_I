\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{img/}}
\usepackage{float}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{subcaption}
% \raggedbottom

%pakker til links og referencer
\usepackage{hyperref}

\makeatletter
\def\@footnotecolor{red}
\define@key{Hyp}{footnotecolor}{%
 \HyColor@HyperrefColor{#1}\@footnotecolor%
}
\def\@footnotemark{%
    \leavevmode
    \ifhmode\edef\@x@sf{\the\spacefactor}\nobreak\fi
    \stepcounter{Hfootnote}%
    \global\let\Hy@saved@currentHref\@currentHref
    \hyper@makecurrent{Hfootnote}%
    \global\let\Hy@footnote@currentHref\@currentHref
    \global\let\@currentHref\Hy@saved@currentHref
    \hyper@linkstart{footnote}{\Hy@footnote@currentHref}%
    \@makefnmark
    \hyper@linkend
    \ifhmode\spacefactor\@x@sf\fi
    \relax
  }%
\makeatother



\hypersetup{colorlinks = true, footnotecolor = black, citecolor = cyan, linkcolor =cyan}


% pakker til c kode
\usepackage{listings}
\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{my_python}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=2pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    language=Python
    % basicstyle = \tiny
}

\def\tablename{Table}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pagestyle{fancy}
\fancyhf{}
\rhead{\today}
\chead{Computer Project I -- Final Report}
\lhead{A. H. S. Poulsen \& A. K. Thomsen}
\rfoot{Page \thepage}

\begin{document}

\title{Computer Project I -- Final Report} 

\author{\IEEEauthorblockN{1\textsuperscript{st} Asger Høøck Song Poulsen }
\IEEEauthorblockA{\textit{Undergraduates at Aarhus University,} \\
\textit{Dept. of electrical and computer engineering}\\
Study number: 202106630\\
AU-id: au704738}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Andreas Kaag Thomsen}
\IEEEauthorblockA{\textit{Undergraduates at Aarhus University,} \\
\textit{Dept. of electrical and computer engineering}\\
Study number: 202105844 \\
AU-id: au691667}
}

\maketitle

\begin{abstract}
    With the continuous improvement of autonomous robots they could sooner than later provide a competetive advantage to more complex tasks such as driving a car or performing a search and rescue operation by reducing error rate, decreasing long-term costs and provide access to dangerous locations. In this paper, we report on our first hands-on experience with embedded programming, working with ROS on a virtual machine and design of real time control software for Aarhus University's computer engineering students. We briefly look into basic Arduino programming and use this knowledge to further implement a program on the Turtlebot3 Burger Robot to drive and avoid obstacles autonomously. We conclude that the implementation of the see-think-act cycle is advantageous for the development of autonomous robots. Furthermore, the course/project has given us as 1st year computer engineering students foundational tools to see, think and act towards developing more complex robots in the future.
\end{abstract}

\section{Introduction}
    Overall, the project can be divided into four parts with certain aim and objectives: 
    \begin{enumerate}
        \item Understanding the basic concepts of robotics programming by working with an Arduino - that is, the \emph{see-think-act cycle} (see figure \ref{fig:See-think-act}). 
        Reading and analyzing data from a range sensor and RGB sensor.
        \item Working with the \emph{Robot Operating System} (ROS) on a virtual machine. Learning the structure that ROS provides and going through the beginner's tutorial. 
        \item ROS programming on Raspberry PI. Programming the robot to avoid obstacles in different scenarios. 
        \item Optimizing the robot's performance both with respect to linear speed and collision avoidance. Finalizing the code and testing the robot on an obstacle course. 
    \end{enumerate}

\section{Specifications}
We have used the following equipment throughout the course. 
\begin{itemize}
    \item Arduino PRO MICRO - 5V/16MHZ 
    \item Ultrasonic Range Finder (LV-MAXSONAR-EZ0)
    \item RGB Light Sensor ISL29125
    \item LED's, cables etc.
    \item Turtlebot3 Burger Robot equipped with i.a. a Raspberry Pi 3 and a 360\textdegree LiDAR sensor (lds-01).  
\end{itemize}
For coding we have used the language C++ for programming the Arduino on the Arduino software. 
For programming the Turtlebot we have been using Python and the command prompt with the built in nano-editor. 

\section{Design and implementation}
    In this section we will describe our thoughts on design and implementation of the project.
    We are only going to desribe the underlying abstract thoughts. The concrete implementation will be elaborated in section \ref{sec:experiment_setup_and_result}.

    \subsection{Part 1: Arduino Programming}
    We were to consider the \emph{see-think-act} cycle as can be seen in figure \ref{fig:See-think-act}:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{see_think_act.png}
        \caption{See-think-act cycle}
        \label{fig:See-think-act}
    \end{figure}
    On our breadboard we connected the Arduino to which we connected the ultrasonic range finders and some LED's. 
    The cycle was then implemented as follows: the range finder sensors \textbf{saw} any obstacle we put in front of it, and sent some data back to Arduino. 
    This data was computed - that is the \textbf{think} step of the cycle. Then the \textbf{act} step was performed, which
    in this case was the LED blinking. \\
    Afterwards we extended the circuit to include three range finder sensors plus the RGB light sensor and four LED's. 
    Thus, this acted as a simulation of the real Turtlebot, which we should work with in part 3. 

    \subsection{Part 2: ROS Programming}
    In this part of the course we worked on a Virtual Machine (VM) on which we had installed Ubuntu. 
    We went through the ROS beginner's tutorial and learned the structure of a ROS system. 

    \subsection{Part 3: Programming the robot to avoid obstacles}\label{subsec: Programming the robot to avoid obstacles}
    As described, the Turtlebot is equipped with a LiDAR 360\textdegree \ laser sensor, which measures the distance.
    It continuosly returns a set of data: 
    \begin{displaymath}
        \texttt{dist} = [d_0, d_1, \dots, d_{359}]
    \end{displaymath}
    where \(d_i\) is the distance to the nearest object at angle \(i\). 
    We decided to look at a span of 120 degrees, which we initially divided into three distinct parts:
    \begin{align*}
        \texttt{left}  &= [d_{15}, d_{16},\dots,d_{60}], \texttt{N=45} \\
        \texttt{front} &= [d_{0},d_{1},\dots,d_{14}, d_{345}, d_{346},\dots, d_{359}], \texttt{N=30} \\
        \texttt{right} &= [d_{300}, d_{301},\dots, d_{344}], \texttt{N=45}
    \end{align*}
    We decided to do this so the robot more often would \emph{turn} instead of just driving \emph{backwards}, 
    since this supposedly would achieve an overall higher linear speed. This will be discussed further in section \ref{sec:discussion}. \par
    For the different cases of obstacles we have made several cases of \texttt{if/elif} statements. 
    The cases and the decisions in each case can be seen in figure \ref{fig:obstacle_cases} below.
    \begin{figure}[H]
        \centering
            \includegraphics*[scale = 0.5]{obstacle_cases.png}   
        \caption{Obstacle cases}
        \label{fig:obstacle_cases}
    \end{figure}
    We thus only considered four distinct cases for our intitial setup. However, we later found that this was not sufficient. Our solution to this will be elaborated in the next section. 

    \subsection{Part 4: Optimizing the robot's performance }
    In the last part of the course we were meant to test the robot on an arbitrary course of obstacles.
    We did, however, test the robot continuosly in part 3, so we focused this part on optimizing the existing code. 
    The goal for the optimization part was to maintain the linear speed as high as possible and the number of collisions as low as possible.
    This will be elaborated in subsection \ref{subsec:partIV_4}.   
     

\section{Experiment setup and results}\label{sec:experiment_setup_and_result}
In this section we will describe the concrete implementation of the project and the results we got, which will
include several code snippets and figures. 

    \subsection{Part 1: Arduino Programming}
    We connected the Range finder sensor to the Arduino and intialized it as shown in the code snippet below:
    \begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Initialization of variables, language = C]        
int SENSOR = A0; //range sensor connected to port A0
double range_input = 0; //variable for storing input values
void setup()
{
  pinMode(SENSOR,INPUT); //sensor declarared as an input
} 
\end{lstlisting} \end{minipage} 
In the control loop we updated \texttt{range\_input} with the value read from the sensor by using the \texttt{analogRead} function.
Through a series of \texttt{if-else} statements we made the LED blink with different intervals for different distances:
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, language = C, caption = \texttt{else-if} statements for Arduino, basicstyle = \footnotesize]
if (range_input+margin < 20 && range_input+margin > 0) {
    TXLED0;
    delay(50);
    TXLED1;
    delay(50);
}

else if (range_input+margin < 30 && range_input+margin > 25) {
    TXLED0;
    delay(333);
    TXLED1;
    delay(333);
} 
    
else if(range_input+margin < 25 && range_input+margin >20) {
    TXLED0;
    delay(1000);
    TXLED1;
    delay(1000);
}
TXLED1; //LED turned off by default  
\end{lstlisting} \end{minipage} 
We were thus able to see the robot \textbf{act} in different ways depending on what it \textbf{saw}, which substantiated the \emph{see-think-act} cycle. \\
Afterwards we extended the circuit to include three range sensors in the same way as above. 
We discovered that our measurements at first were wrong since we did not convert the input values to distances. 
Looking at the equipment specifications we found out that the sensor uses a scaling factor of \(6.4 \frac{mV}{inch}\). 
By dividing the analog input with this scaling factor, a measurement in inches is calculated. Afterwards, this is multiplied by \(2.54 \frac{cm}{inch}\).
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Calculations of ranges, basicstyle = \footnotesize]
front_input_cm = (front_input/6.4)*2.54;
left_input_cm = (left_input/6.4)*2.54;
right_input_cm = (right_input/6.4)*2.54;
\end{lstlisting} \end{minipage} 
Now we had the correct measurements which we also manually confirmed with a ruler. \\
Lastly we also implemented the RGB light sensor on the breadboard, which was intialized in the same way as the range sensors. 
The RGB sensor returned one value measured in lux for the luminance. If the luminance was less than 30,
one LED was turned on. With everything connected as described above, our final circuit is depicted in figure \ref{fig:arduino_final} below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\textwidth, angle = 90]{arduino_final.jpeg}
    \caption{Final Arduino circuit}
    \label{fig:arduino_final}
\end{figure}
We now had a circuit that would serve as the building blocks for our final implemenatation of the Turtlebot with each range finder sensor acting as a representative of each slice desribed in subsection \ref{subsec: Programming the robot to avoid obstacles}.
We will elaborate this further in subsection \ref{subsec:arduino_as_blocks}. 

    \subsection{Part 2: ROS Programming}
    As mentioned we followed the ROS beginner's tutorial in which we installed ROS using several commands. 
    We got stuck in part 2 of the tutorial, so we decided to go back to the beginning and reinstall everything.
    After doing this, we were able to complete all the steps from 1 through 17. 
    We got a deeper understanding of how the ROS system is structured and learned several ROS-related commands. 
    We will explain the most essential for our further work with the Turtlebot below\footnote{These definitions are taken from the ROS beginner's tutorial \cite{b2}}:
    \begin{enumerate}
        \item \textbf{roscore}: roscore is a collection of nodes and programs that are pre-requisites of a ROS-based system. You must have a roscore running in order for ROS nodes to communicate.
        \item \textbf{rosrun}: rosrun allows you to run an executable in an arbitrary package from anywhere without having to give its full path. 
        \item \textbf{source}: When opening a new terminal your environment is reset and you are obliged to use the \texttt{source} command to re-source the .bashrc file. 
        \item \textbf{nano}: nano is a text editor that is used to edit files. We used it to edit our code files directly in the terminal.
        \item \textbf{catkin\_make}: catkin\_make is a convenience tool for building code in a catkin workspace. You should always call catkin\_make in the root of your catkin workspace
    \end{enumerate}
    In practice we only needed to run three commands after compiling the program using \texttt{catkin\_make} when the robot should be tested.
    Firstly we should run \texttt{source ~/.bashrc} in each new terminal to make sure the environment is set up correctly.
    Afterwards we needed to run \texttt{roslaunch turtlebot\_bringup\ turtlebot\_bringup.launch} to start the ROS-master (\texttt{roscore}). 
    Then, in a different terminal we should execute the code (node), we had written ourselves by writing the command: \texttt{roslaunch turtlebot3\_example\ turtlebot3\_obstacle.launch}.
    
 
    \subsection{Part 3: Programming the robot to avoid obstacles}

    \subsubsection{Using the Arduino setup as building blocks}\label{subsec:arduino_as_blocks}
    
    

    In this part we are going to describe our initial implementation. The optimization we did, will be described in part 4. \\
    We were given a robot navigation example from a GitHub repository\footnote{\cite{b6}} from where we were to get inspiration and further implement our own robot navigation. The idea of the initial example was to: \emph{Drive forward until we get close to an obstacle, then stop}. The robot kept driving by initiating the following \texttt{while}-loop
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = control loop, basicstyle = \footnotesize]
while not rospy.is_shutdown():
\end{lstlisting} \end{minipage}
    So \texttt{while} we keep publishing the topic, we iterate through the following \texttt{else-if} statements that should make the robot react if the cases from figure \ref{fig:obstacle_cases} were to be encountered. \\
    For case \(1\) we initially programmed the robot to back out of the blind alley and then turn around:
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Case 1, basicstyle = \footnotesize, label=lst:initial_case_1]
if (right_distance < SAFE_STOP_DISTANCE and left_distance < SAFE_STOP_DISTANCE and front_distance < SAFE_STOP_DISTANCE):
    if turtlebot_moving:
        twist.linear.x = -LINEAR_VEL
        twist.angular.z = 0.0
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning backwards for 1 second')
        t.sleep(1.0)
        twist.linear.x = 0.0
        twist.angular.z = 3.14/2
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning around')
        t.sleep(1.0)
        turtlebot_moving = False
\end{lstlisting} \end{minipage}
    Below here we will explain this code line for line. Note that each item corresponds to each line number in the listing. 
    \begin{enumerate}
        \item Requirements for the specific case. 
        \item This variable was set to \texttt{true} each time the robot was driving forward. The robot would thus only \textbf{act} if it was not already turning away from some other obstacle. 
        \item In order to alter the robot's movement we had to update the \texttt{twist}-command. In this line we updated the linear velocity, which should be negative for the robot to move backwards.
        \item Here, we update the twist command with respect to the angular velocity which in this case should be zero since the robot should \emph{only} drive backwards. 
        \item In this line we \emph{publish} the newly updated \texttt{twist}-command to the turtlebot. When we do this, the robot will act accordingly. 
        \item We write to the log what we are doing. 
        \item We wanted the robot to turn backwards for some time, in this case 1 second. 
        \item Now the robot should only turn and we therefore set the linear speed to zero.
        \item The robot should turn in place, so we set the angular speed to \(\pi /2\).
        \item Publishing the new twist command.
        \item Writing to the log. 
        \item Wait for 1 second. 
        \item Now we set the beforementioned variable \texttt{turtlebot\_moving} to \texttt{false}. It will \emph{only} be set to \texttt{true} again when the robot should drive forward (shown in listing \ref{lst:drive_forward}). 
    \end{enumerate}
    These explanations also hold for all other cases which for this reason will only be described briefly below. 

    For our remaining cases we used a function called \texttt{small\_turns} as shown in listing \ref{lst:small_turns} to control the robot. 
    The function's functionality corresponds exactly to the steps desribed above - it has only been generalized. 
    The function takes three arguments; a direction, an angular velocity, and a linear velocity which are all used to control the robot\footnote{For our initial implementation we only used the two first inputs and the linear velocity when turning was consistenly equal to \(0.1 \frac{m}{s}\).}. 
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = \texttt{small\_turns} function., basicstyle = \footnotesize, label = lst:small_turns]
def obstacle(self):
twist = Twist()
turtlebot_moving = True

def small_turns(dir, angle, vel):
    if(dir == 'left'):
        twist.linear.x = vel
        twist.angular.z = angle
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning left')
        turtlebot_moving = False

    elif (dir == 'right'):
        twist.linear.x = vel
        twist.angular.z = -angle
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning right')
        turtlebot_moving = False
\end{lstlisting} \end{minipage}

    For case \(2\): If the robot found an obstacle to the front it was programmed to look to the left and right respectively, look for next potential obstacle and then turn in the opposite direction of the next potential closest obstacle:
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Case 2, basicstyle = \footnotesize]
elif (front_distance < SAFE_STOP_DISTANCE + 0.05):
    if turtlebot_moving:
        if (right_distance < left_distance):
            small_turns('left', turn_angle + 0.2)
        elif(left_distance < right_distance):
            small_turns('right', turn_angle + 0.2)
\end{lstlisting} \end{minipage} 
    For case \(3\) and \(4\): If an obstacle to the left is within the \texttt{SAFE\_STOP\_DISTANCE} turn right and vice versa.
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Case 3 \& 4, basicstyle = \footnotesize]
elif(left_distance < SAFE_STOP_DISTANCE or right_distance < SAFE_STOP_DISTANCE):
    if turtlebot_moving:
        if(right_distance < left_distance):
            small_turns('left', turn_angle)
        else:
            small_turns('right', turn_angle)
\end{lstlisting} \end{minipage} 
    Lastly, by default, we drive forward if there are no obstacles to avoid. Note that this is where we set \texttt{turtlebot\_moving} to \texttt{true}. 
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Else: drive forward, basicstyle = \footnotesize, label = lst:drive_forward]
else:
    twist.linear.x = LINEAR_VEL
    twist.angular.z = 0.0
    turtlebot_moving = True
    self._cmd_pub.publish(twist)
\end{lstlisting} \end{minipage} 
    Thus, ending an iteration of the control-loop \texttt{while not rospy.is\_shutdown():}.

        \subsubsection{Retrieving data from the LiDAR sensor}
            In the original GitHub repository, the LiDAR sensor only looked at one measurement. We altered the function \texttt{get\_scan} to fit our needs:
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = \texttt{get\_scan} function, basicstyle = \footnotesize, label = lst:non_zeros]
def get_scan(self):
    scan = rospy.wait_for_message('scan', LaserScan)
    scan_filter = []

    left_lidar_samples_ranges = 60
    right_lidar_samples_ranges = 345
    front_lidar_samples_ranges = 15

    front_lidar_samples1 = scan.ranges[0:front_lidar_samples_ranges]
    front_lidar_samples2 = scan.ranges[right_lidar_samples_ranges:360]
    left_lidar_samples = scan.ranges[front_lidar_samples_ranges:left_lidar_samples_ranges]
    right_lidar_samples = scan.ranges[300:right_lidar_samples_ranges]

    scan_filter.append(left_lidar_samples)
    scan_filter.append(front_lidar_samples1)
    scan_filter.append(front_lidar_samples2)
    scan_filter.append(right_lidar_samples)

    return scan_filter
\end{lstlisting} \end{minipage} 
    We recieve the LiDAR data when calling \texttt{rospy.wait\_for\_message} and then create an empty list \texttt{scan\_filter} from which we would fill in the data that we would need, hence \textbf{filter} out the unecessary data, since we are only interested in distances in a span of \(120\) degrees as mentioned in subsection \ref{subsec: Programming the robot to avoid obstacles}. From there we append the sections we are interested in to the list \texttt{scan\_filter} with Python's \texttt{append} function. When doing so, \texttt{scan\_filter} consists of four elements which are each tuples of distances. 
    \subsubsection{Tuples in python} When testing the robot we encountered that the LiDAR would return a lot of zeros which would represent an obstacle too close to the robot such that the robot would and should react to. This is because the LiDAR sensor has a max range of \(3.5\) m\footnote{\cite{b8}}, so if an obstacle was further away than this it would just set the distance data to \(0\) which was a problem. We solved this by converting the zeros to \(3.5\) (max distance) instead. However, in Python one can not edit data in tuples, so we had to convert this list of tuples to a list of lists and from there convert the zeros to \(3.5\). We created the following function \texttt{non\_zeros} to do so.

\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = \texttt{non\_zeros} function, basicstyle = \footnotesize, label = lst:non_zeros]
#convert list of tuples to list of lists, and then convert zeros to 3.5:
def non_zeros(l):
    # convert tuples to list
    for i in range(len(l)):
        l[i] = list(l[i])

    # if element zero, set to 3.5
    for i in range(len(l)):
        for j in range(len(l[i])):
            if l[i][j] == 0:
                l[i][j] = 3.5
\end{lstlisting} \end{minipage} 
    The first for-loop converted the tuples to lists and the next converted zeros to \(3.5\).

    \subsubsection{A short note on the \emph{see-think-act cycle}}
    Considering our code above on a more abstract level, we can see that this actually corresponds 1:1 with the see-think-act cycle as seen in figure \ref{fig:See-think-act}. 
    Our \texttt{get\_scan}-function thus acts as the \textbf{see}-step in which we 'sense' our environment, so to speak. 
    Afterwards we have the \textbf{think}-step in which we determine the best action to take. That is, the different \texttt{if/elif}-statements. 
    Finally, we have the \textbf{act}-step in which we execute the action we have decided upon. This is when we publish our \texttt{twist}-command to the Turtlebot. 

    \subsubsection{Defining the macros}
    Several of the code snippets above shows a bunch of global variables that we had initialized in the very beginning of our code. 
    These initializations can be seen in listing \ref{lst:global_variables}.
    \begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Global variables, basicstyle = \footnotesize, label = lst:global_variables]
LINEAR_VEL = 0.22
STOP_DISTANCE = 0.2
LIDAR_ERROR = 0.05
SAFE_STOP_DISTANCE = STOP_DISTANCE + LIDAR_ERROR 
MAX_ANGLE = 2.84
DEFAULT_TURN_ANGLE = 3.14/2  \end{lstlisting} \end{minipage}     
Now, we are going to explain these choices. Note that each item corresponds to each line in listing \ref{lst:global_variables}.
\begin{enumerate}
    \item Since we wanted the highest possible linear velocity, we set the default value of \texttt{LINEAR\_VEL} to \(0.22 \frac{m}{s}\), which is the maximum linear velocity of the turtlebot according to its documentation\footnote{\cite{b7}}. 
    This was the linear speed the robot would move at when driving forward. When turning, this would be adjusted with respect to this basis. 
    \item We chose this by empirically testing which distance the robot could avoid an obstacle by smoothly turning. We found that with smaller distances, 
    the robot had to stop completely, which we only wanted in critical cases. 
    \item This error was the one defined in the initial code setup from GitHub\footnote{We later learned that this error was actually not correct according to the specifications of the LiDAR sensor. The accuracy for measurements in the range \(120 \text{mm} - 499 \text{mm}\) was \(\pm 15\)mm. However, we experienced good results with the chosen values.}\label{footnote:lidar_accuracy}.
    \item This is the (default) distance we would actually stop at. 
    \item This is the maximal angular velocity measured in \(\frac{rad}{s}\) according to the specifications of the turtlebot\footnote{\cite{b7}}. 
    \item This is the angular velocity the robot would turn at when turning in most cases.
\end{enumerate}



    \subsection{Part 4: Optimizing the robot's performance }\label{subsec:partIV_4}
    In this section we are going to describe the changes we applied to the code setup from part 3 and the results we got. \\
    \subsubsection{Adding a \texttt{factor} to \texttt{SAFE\_STOP\_DISTANCE}}\label{subsec:factor}
    We experienced that when the robot encountered an obstacle on its front it did not manage to avoid it optimally. 
    To solve this problem we added a factor of \(0.08\)cm to the \texttt{SAFE\_STOP\_DISTANCE} constant in this case. By doing this, the robot began avoiding the obstacle earlier, 
    and thus it was able to avoid it more optimally. Same goes for many of our other cases.\\
    
    \subsubsection{Making sure to avoid the obstacle before next control loop iteration}\label{subsec:avoid_obstacle}
    We wanted to make sure that the robot actually avoided the obstacle before the next control loop iteration.
    We did this by adding a \texttt{while}-loop to each case as shown in listing \ref{lst:while_loops}.
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = \texttt{while}-loops for obstacle avoidance, basicstyle = \footnotesize, label=lst:while_loops]
# case 1: obstacle in front
if (front_distance < SAFE_STOP_DISTANCE + 0.08):
    print('obstacle in front')
    # look ahead:
    if turtlebot_moving:
        if (right_distance < left_distance):
            while(front_distance < SAFE_STOP_DISTANCE + 0.08):
                small_turns('left', 'angular vel', 'linear vel.')
                front_new = self.get_scan()
                non_zeros(front_new)
                emergency_check(front_new)
                front_distance = np.mean(front_new[1] + front_new[2]) \end{lstlisting} \end{minipage} 
    By doing this we ensured that the robot would not stop avoiding the obstacle before it had actually escaped it\footnote{Note that the 2nd and 3rd argument in the \texttt{small\_turns} function are written in pseudo-code since we did further optimizations, which will be described in subsection \ref{subsec:optimize_linear_speed}. 
    The function \texttt{emergency\_check} will also be described in subsection \ref{subsec:emergency_check}.}.
    For each loop iteration we made a new laser scan and checked if the robot was within the \texttt{SAFE\_STOP\_DISTANCE+factor}. If it is, we called the \texttt{small\_turns} function,
    and turned the robot in the opposite direction of the closest obstacle as normally.     

    \subsubsection{Checking the blind angles}
    In addition to the three directions we had already implemented, we decided to focus on two more directions; the so-called blind angles.
    These were meant to make the robot able to avoid an obstacle if it was only slightly in front of it, but not so much that the average would get low enough to cause a turn event. 
    We calculated these by measuring the minimal distance between two obstacles that robot could drive through - that is, the width of the robot, which we measured to be \(20\)cm.
    We wanted the robot to begin avoiding the obstacles at the blind angles from a distance of \(40\)cm. We got the results using GeoGebra as depicted in figure \ref{fig:blind_angle}. 
    We also wanted these angles to be very narrow such that the robot would be able to detect even a small corner of some obstacle. For this reason we only looked at a span of \(8\) degrees. 
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{blind_angle.png}
    \caption{Blind angles}
    \label{fig:blind_angle}
    \end{figure}
    Thus, we implemented the following code in our function \texttt{get\_scan()}, which took these new angles into account:
    \begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Blind angles in \texttt{get\_scan()}, basicstyle = \footnotesize]
front_left_lidar_samples = scan.ranges[14:22]
front_right_lidar_samples = scan.ranges[338:346]

scan_filter.append(front_left_lidar_samples)
scan_filter.append(front_right_lidar_samples) \end{lstlisting} \end{minipage} 
    In addition to this, we added an extra case (named case 3) for checking these blind angles. This was implemented in the exact same way as desribed in subsection \ref{subsec:avoid_obstacle} using another \texttt{elif}-statement. Note that the \texttt{scan\_filter} list now consists of \(6\) tuples. 
    The robot should detect these blind angles quite early, so we added a factor of \(0.15\)cm to the \texttt{SAFE\_STOP\_DISTANCE} constant. \\

    \subsubsection{Optimizing linear speed}\label{subsec:optimize_linear_speed}
    For our initial implementation, our robot would \emph{turn} with a constant linear speed of \(0.1 \frac{m}{s}\). Our goal was to achieve an overall higher linear speed.
    We did this by considering the relationship between the linear speed and distance to object as well as the relationship between the angular speed and distance to object.
    We wanted the following:
    \begin{itemize}
        \item The linear speed should be \textbf{smaller}, the closer the obstacle was.
        \item The angular speed should be \textbf{greater}, the closer the obstacle was.
    \end{itemize}
    To do this, we considered some distinct cases\footnote{These speeds were tested empirically until we found appropriate values.}:
    \begin{enumerate}
        \item Robot is at maximal \texttt{SAFE\_STOP\_DISTANCE} away from the obstacle, that is \(0.4\)m for blind angle obstacles and \(0.33\)m for front obstacles.
        \begin{itemize}
            \item Linear speed should be \texttt{MAX\_LINEAR\_VEL}, that is \(0.22 \frac{m}{s}\) for blind angle obstacles and \(0.18 \frac{m}{s}\) for front obstacles.
            \item Angular speed should be minimal, that is \(0.79 \frac{rad}{s}\) for blind angle obstacles and \(2.0 \frac{rad}{s}\) for front obstacles. 
        \end{itemize}
        \item Robot is critically close to the obstacle, that is \(0.1\)m. 
        \begin{itemize}
            \item Linear speed should essentially be zero, that is \(0.01 \frac{m}{s}\).
            \item Angular speed should be \texttt{MAX\_ANGLE}, that is \(2.84 \frac{rad}{s}\). 
        \end{itemize}
    \end{enumerate}
    With these values, we made power regression using GeoGebra. The result for linear velocity is depicted in figure \ref{fig:lin_vel_graph}. 
    Here, \(h(x)\) and \(g(x)\) are the linear velocities in \(\frac{m}{s}\) for front obstacles and blind spots, respectively. \(x\) is the distance to the obstacle in m.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{lin_vel_graph.png}
            \caption{Linear velocities graphs}
            \label{fig:lin_vel_graph}
        \end{figure}
    The result for angular velocity is depicted in figure \ref{fig:ang_vel_graph}. Here, \(g(x)\) and \(h(x)\) are the angular velocities in \(\frac{rad}{s}\) for front obstacles and blind spot obstacles, respectively.
    \(x\) is the distance to the obstacle in m.        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{ang_vel_graps.png}
            \caption{Angular velocities graphs}
            \label{fig:ang_vel_graph}
        \end{figure}
    Afterwards, we implemented these functions in our code. We needed to include the power functions we had found each time we called the \texttt{small\_turns} function. 
    An example for the blind spot case is shown in listing \ref{lst:blind_spot}.
\begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Power functions implemented for blind spot, basicstyle = \footnotesize, label = lst:blind_spot]
small_turns('right', 0.34*front_left_distance**
(-0.93), 1.7*front_left_distance**2.23)
\end{lstlisting} \end{minipage} 
    After implementing these changes in the code, we significantly improved the robot's linear speed. \\

    \subsubsection{Better collision avoidance}\label{subsec:emergency_check}
    Another goal of our optimization was to avoid collisions with obstacles completely. We did this by implementing a new function called \texttt{emergency\_check} that would check if the robot was critically close to an obstacle. 
    We defined the robot to be critically close to an obstacle if it is within \(0.1\)m of an obstacle. With a \texttt{LIDAR\_ERROR} of \(0.05\)m, we introduced a new global variable \texttt{EMERGENCY\_STOP\_DISTANCE = 0.15}. 
    We chose this value since the minimum distance the LiDAR sensor can measure is \(0.12\)m\footnote{\cite{b8}}. We did not want to get below this value since it could result in a faulty reading.\footnote{We would not actually get below this value with the true error of the sensor. See footnote \ref{footnote:lidar_accuracy}.}.
    If it was within this distance, the robot's linear speed would be set to zero and the robot would turn with max angular speed. The function is depicted in listing \ref{lst:emergency_check}.
    \begin{minipage}{\linewidth} \begin{lstlisting}[style = my_python, caption = Emergency check, basicstyle = \footnotesize, label = lst:emergency_check]
def emergency_check(l):
    # calculate mean of our slices:
    left_distance = np.mean((l[0]))
    front_distance = np.mean(l[1] + l[2])
    right_distance = np.mean(l[3])
    front_left_distance = np.mean(l[4])
    front_right_distance = np.mean(l[5])

    # calculate mean of special case slices
    special_case_right = np.mean(l[3][27:35])
    special_case_left = np.mean(l[0][10:18])

    if (front_distance < EMERGENCY_STOP_DISTANCE and left_distance < EMERGENCY_STOP_DISTANCE):
        small_turns('right', MAX_ANGLE, 0.0)

    elif (front_distance < EMERGENCY_STOP_DISTANCE and right_distance < EMERGENCY_STOP_DISTANCE):
        small_turns('left', MAX_ANGLE, 0.0)

    elif (front_distance < EMERGENCY_STOP_DISTANCE):
        if (left_distance < right_distance):
            small_turns('right', MAX_ANGLE, 0.0)
        else:
            small_turns('left', MAX_ANGLE, 0.0)

    elif(special_case_right < SAFE_STOP_DISTANCE and special_case_left < SAFE_STOP_DISTANCE):
        make_180()

    elif(front_left_distance < SAFE_STOP_DISTANCE):
        small_turns('right', MAX_ANGLE, 0.0)

    elif(front_right_distance < SAFE_STOP_DISTANCE):
        small_turns('left', MAX_ANGLE, 0.0) \end{lstlisting} \end{minipage} 

    The function takes an input \texttt{l} which is, as mentioned, a list of tuples containing the latest readings. 
    We called this function for each loop iteration, both in the overall control loop as well as in the loops described in subsection \ref{subsec:avoid_obstacle} and depicted in listing \ref{lst:while_loops}.
    By doing this we constantly checked if the robot was so close to an obstacle that it could not avoid it by turning conventionally. 
    Instead, the robot should turn \emph{in place} with maximal angular speed. Note that we initially wanted the turtlebot to drive backwards and then turn, but while testing the implementation from listing \ref{lst:initial_case_1} we noticed that backing out was not an option unless we broadened our vision span to also look behind, since we could risk backing into an obstacle as illustrated below.
    \begin{figure}[H]
        \centering
            \includegraphics*[scale = 0.4]{Test_fail_example_1.png}   
        \caption{Initial implementation flaw}
        \label{fig:Test_fail_example_1}
    \end{figure}
    So by turning in place until the coast is clear turned out to be an optimal solution for this.
    \begin{figure}[H]
        \centering
            \includegraphics*[scale = 0.4]{test_fail_example_1_solution.png}   
        \caption{Illustration of line 25-26 in listing 16}
        \label{fig:test_fail_example_1_solution}
    \end{figure}
\section{Discussion}\label{sec:discussion}
In this section of the report we will discuss some of the difficulties we encountered during the project.
We are also going to discuss whether or not we reached our goal.
Again, we have divided it into the four main parts of the project described earlier. 

    \subsection{Part 1: Arduino Programming}
    \begin{enumerate}
        \item \textbf{Computing the readings correctly} \\
        We experienced some difficulties in computing the readings correctly since we were not able to find the right scaling factor. 
        We found that many sensors were to be treated differently since they did not give consistent readings. However, since this was only a
        preliminary exercise to the main project, we decided not to spend much time on the problem, since we had to focus on our main goal. \\
        \item \textbf{Preparation to main project} \\
        Working with the Arduino and the appurtenant sensors helped us understand the importance of the see-think-act cycle as shown in figure \ref{fig:See-think-act}.
        Our code structure turned out to be predominantly alike in the way we worked with the \texttt{if/elif}-statements. 
        This was of course only with respect to some very simple cases, but the underlying thought processes were not very different. 
        The body of the \texttt{if/elif}-statements was also way more complex in our robot's code. 
        Moreover, we found that working with the sensors was not that easy and that it demanded a lot of effort and reading to actually understand the input from the sensors.
        This was a valuable lesson that prompted us to work more structured in the main project.  \\
    \end{enumerate}


    \subsection{Part 2: ROS Programming}
    \begin{enumerate}
        \item \textbf{Understanding the ROS structure} \\
        Working with the ROS beginner's tutorial gave us a deeper understanding of how the ROS works.
        We found that in order to get the robot to work we had to follow this structure that ROS provides. 
        Deviation from this would quickly cause the robot to stop working and a bit of troubleshooting would be needed. 
        On a more abstract level we learned that robot programming is extremely complex and requires a lot of prior knowledge, which we do not have the foundation for yet. 
        Given the amount of time allocated to this project, we decided to focus on the more important aspects, which would be the programming and optimizations of the robot. 
        We did manage to learn many essential aspects of ROS and used these skills in our work with the Turtlebot.\\
    \end{enumerate}


    \subsection{Part 3: Programming the robot to avoid obstacles}
    \begin{enumerate}
        \item \textbf{Getting wrong sensor measurements} \\
        We experienced quite a difficulty in getting the right measurements from the sensor.
        We knew that it would return an array of size 360 with one distance for each angle. 
        However, we couldn't figure out how they were indexed and our robot thus behaved incorrectly and sometimes
        turned left when it should turn right.
        We solved the problem by manually printing out different test angles and then physically measure that angle also. 
        For instance printing \texttt{scan\_ranges[45]} gave some change in output distance for some specific angle, which we measured. 
        It turned out that we had used it incorretly and the indexing was actually opposite of what we had thought. 
        The indexing was counterclockwise, such that angle \(0-90\) degrees was the first 90 degrees to the left.  \\
        
        \item \textbf{Using \texttt{mean} instead of \texttt{min} and resolving problems with faulty readings} \\
        Our initial code setup from GitHub used Python's \texttt{min} function to find the representative for each direction's minimum distance.
        However, we found that our readings from the LiDAR contained a lot of zeros, which caused this representative to be zero very often.
        For this reason we decided to calculate the average of the readings instead using Numpy's \texttt{mean} function.
        After doing this we experienced that our readings became much more representative of the actual distance.\\
        Moreover, since our readings still contained a lot of zeros, we modified our code to use the \texttt{non\_zeros}-function as shown in listing \ref{lst:non_zeros}.
        This converted potential zeros to the sensor's max range of \(3.5\)m. 
        This could however actually become a problem since both very close obstacles and very far obstacles could be represented by the same distance of 0.
        Thus our average value could become quite ambiguous. A better way to deal with this problem could be to \emph{remove} these faulty reading, making the average more precise and more reliable. \par
        When using \texttt{min} we also only concentrate on a single point in our 'slices', which when we tested the LiDAR sensor was not always 100\% accurate, so sometimes when there actually was a closer object to the left than to the right the robot would still too often turn left. Since \texttt{mean} considers all the points in the 'slices' we get a more representative view of the current situation the robot should react to.\\
        
        \item \textbf{Finding the right span}\label{subsec:Finding the right span} \\
        In the beginning of the robot programming, we only looked at \emph{point readings} and not an entire span of readings. 
        Of course, this resulted in a lot of problems, since we could not see most obstacles. Thus, we had to find a proper span of angles to look at.
        As mentioned, we ultimately decided to look at a span of \(120\) degrees, which is not coincidental. 
        Figure \ref{fig:span} shows a graphic representation of the chosen span, which we will argue for. 
        \begin{figure}[H]
            \centering
                \includegraphics[width=0.45\textwidth]{120_degrees_argumentation.png}
                \caption{The chosen span of the robot's sensor-readings.}
                \label{fig:span}
        \end{figure}
        First of all, the span should be less than \(180\) degrees, as the robot should \emph{not} try to avoid obstacles that it would not drive into by continuing driving forward. 
        Secondly, the span should be large enough to cover the entire field of view of the robot.
        If we only looked at, say, \(30-50\) degrees ahead, the span was not wide enough to differentiate between different directions,
        and thus choose in which direction to turn. 
        With the choice of \(120\) degrees, we had a complete view of the field of view of the robot and achieved the following:
        \begin{itemize}
            \item The robot could see obstacles that was \emph{directly} in front of it. 
            In these cases, it would also be able to select whether to turn left or right. 
            We chose the front span to be only \(30\) degrees wide such that it could also avoid very narrow obstacles in front of it. 
            \item The robot could see obstacles that was \emph{indirectly} in front of it. That is, to the left \emph{or} right of it. 
            In these cases, it would only turn slightly to the left or right with high linear speed and thus almost maintain its path. 
            \item In the special case that the robot was facing just a corner of an obstacle, it would also see this due the 'blind angle' sections. 
            Again, the robot would only turn very slightly to the left or right with high linear speed and thus practically maintain its path.
        \end{itemize}
        The \texttt{SAFE\_STOP\_DISTANCE}s in figure \ref{fig:span} are due to the factors introduced in subsection \ref{subsec:factor}.
    \end{enumerate}

    \subsection{Part 4: Optimizing the robot's performance }
    \begin{enumerate}
        \item \textbf{Structuring the \texttt{if/elif} statements} \\
        During our implementation we found that the way we ordered our \texttt{if/elif}-statements was very important to pay attention to.
        At first, we did not really pay attention to this, but we experienced that our robot was not working optimally and kept driving into obstacles.
        We discussed that it was most important that the robot avoided obstacles that were directly in front of it.
        If it did anything else at first, it would often not turn enough and thus drive into the front obstacle. 
        Next we wanted in to avoid obstacles that were to the left or right, since this case was defined with a higher angular velocity than the blind angle cases. 
        Finally we wanted the robot to slightly turn away from obstacles that were in the blind spots of the robot. 
        In this way the robot would always make the \emph{most radical choice if it was necessary.} Next it would go to the second-most radical choice and so on.
        If nothing was needed to be done, it would simply drive forward. \\

        \item \textbf{Finding the proper regression type} \\
        As mentioned, we used power regression to determine the relationship between the distance and the speed (angular and linear).
        But why did we choose this instead of a linear regression? Or exponential regression? Linear regression would indisputable result in a higher average linear speed (see figure \ref{fig:regression_types}).
        We found that if the robot was driving too fast while avoiding an obstacle it would eventually drive into the obstacle.
        We thus wanted a regression type, which would slow down the robot quite early, avoid the obstacle and then quickly accelerate again.
        It should, however, not slow down the robot too much (as the exponential fit does) since we still wanted a high average linear speed. For these reasons we chose power regression for linear velocity (the green curve in figure \ref{subfig:regression_types_linear}).  
        \begin{figure}[H]
            \centering
            \begin{subfigure}{.5\textwidth}
              \centering
              \includegraphics[width=.5\linewidth]{regression_types_linear.png}
              \caption{Linear velocity}
              \label{subfig:regression_types_linear}
            \end{subfigure}
            \begin{subfigure}{.5\textwidth}
              \centering
              \includegraphics[width=.5\linewidth]{regression_types_angular.png}
              \caption{Angular velocity}
              \label{subfig:regression_types_angular}
            \end{subfigure}
            \caption{Comparison of three different regression types.}
            \label{fig:regression_types}
        \end{figure}
        When it comes to the angular velocity we wanted the robot to only turn very slightly if it was far away from the obstacle. 
        This was because it should essentially maintain its current path if possible. 
        We only wanted it to turn with max angular speed if it was critically close to the obstacle (around \(15\)cm). 
        Looking closely at our chosen curve (the green one in figure \ref{subfig:regression_types_angular}), we see a very steep slope around the critical distance.
        Further mathematical analysis revealed that the slope of the green curve was actually the greatest at \(x=0.15 \text{cm} \pm 0.02\)cm.\footnote{The slope of these curves is naturally the acceleration since differentiation of the velocity gives the acceleration.}
        The actual calculations can be seen in table \ref{tab:slope_calculations}.  
        \begin{table}[H]
        \centering
            \resizebox{0.4\textwidth}{!}{
            \begin{tabular}{|l|cll|}
            \hline
                                & \multicolumn{3}{c|}{\textbf{Distance (measured in meters)}} \\ \hline
            {\textbf{Regression type}} & \multicolumn{1}{c|}{\(x = 0.13\)} & \multicolumn{1}{l|}{\(x = 0.15\)} & \(x = 0.17\) \\ \hline
            Power regression       & \multicolumn{1}{c|}{\(-16.22\)}  & \multicolumn{1}{l|}{\(-12.3 \)} & \(-9.66\)\\ \hline
            Exponential regression & \multicolumn{1}{c|}{\(-10.71\)}  & \multicolumn{1}{l|}{\(-9.83 \)} & \(-9.02\)\\ \hline
            Linear regression      & \multicolumn{1}{c|}{\(-6.85\)}  & \multicolumn{1}{l|}{\(-6.85 \)}  & \(-6.85\)\\ \hline
            \end{tabular} }
            \caption{Slope of curves from figure \ref{fig:regression_types} around critical distance.}
            \label{tab:slope_calculations}
        \end{table}
        In this way the robot would only make a sharp turn when the obstacle was critically close to an obstacle. The acceleration would be the greatest at the critical distance, which we wanted. 
        Consequently we chose power regression for the angular velocity as well. \\

        \item \textbf{Obstacle avoidance for some special cases} \\
        We have already argued that the robot would avoid an obstacle even if it was just facing a corner of it, though we emphasized the fact that it would be on \emph{either} of the sides. 
        However, we did not completely succeed in solving the problem of an obstacle in both blind spots. 
        We did calculate these angles such that it would be able to drive through if there was nothing in both blind spots.
        But we experienced that this only worked when the robot was driving completely straight towards these blind angles.
        We discussed that the problem might could have been solved by widening the gap between the blind spots and thus having larger safety margin. 
        However, this would compromise the original thought behind these angles. Perhaps a better solution would be to introduce a new case having its own parameters considering this situation. 
        The best approach would be to make the 'slices' of the blind spots dependent on the distance to to the obstacles, since we measured the angles as depicted in figure \ref{fig:blind_angle}. \\
        
        \item \textbf{Is it possible to avoid the emergency stop?} \\
        As mentioned in subsection \ref{subsec:avoid_obstacle} we implemented a function called \texttt{emergency\_check} which would check if the robot was in an emergency situation.
        If so, it would set the linear velocity to zero and the angular velocity to the maximum possible. However, one could argue that an optimal implementation (with respect to linear speed) would not need this. 
        It is important to consider the trade-off between linear speed, obstacle avoidance, and areal covered\footnote{Our motivation for the project was namely to simulate a search and rescue robot, which could both avoid obstacles \emph{and} find victims. For this reason it is very important to cover as much area as possible. The 'rescue' part was discarded during the course due to unknown reasons.}. 
        Assume that we set the \texttt{SAFE\_STOP\_DISTANCE} very high, perhaps \(1.0\)m. Then the robot would act very early and easily avoid the obstacle. 
        But this would also have the effect that the robot would not be able to drive close to the obstacle. 
        Ultimately we wanted a robot that could drive close to an obstacle and still be able to avoid it maintaining a high average linear speed.
        This caused the robot to sometimes be very close to an obstacle, which prompted that the linear speed had to be set to zero for a moment. 
        An implementation which should \emph{only} be optimized with respect to obstacle avoidance and linear speed could avoid this and just drive away from potential obstacles in good time with maximal linear speed.
        This was not the goal of our implementation and so we chose to have the emergency-function. \\

        \item \textbf{Final tests of the robot} \\
        We tested the robot with a various of different scenarios. Illustrated below are two of the many test-courses on which we tested the final implementation on.
    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\linewidth]{IMG_3307.jpg}
        \caption{Navigating through tight spaces}
        \label{subfig:tight_spaces}
    \end{figure}
    We encountered that the turtlebot would mostly navigate easily through tight spaces. There were some few occasions where the wheels would graze an obstacle. Since there are obstacles in almost every direction these were the type of test-courses we found to be the most difficult with the most obstacle collisions. We also made the most significant breakthroughs in optimizing the navigation when testing through tight spaces. It was here we implemented the \texttt{emergency\_check} function which reduced the amounts of collisions dramatically.
    \begin{figure}[H]
        \centering
        \includegraphics[width=.6\linewidth]{IMG_3308.jpg}
        \caption{Placing obstacles in random places}
        \label{subfig:random_placed_obstacles}
    \end{figure}
    The final implementation navigated through this type of test-course smoothly and without any collisions. It was through these types of test-courses where the optimization of keeping the linear speed as high as possible with regressional input to the \texttt{small\_turns} function was implemented, which had a huge impact on the linear velocity. 
\end{enumerate}

    \section{Conclusion}
    The see-think-act cycle is a powerful tool in understanding basic robot behaviour. 
    Though working with quite simple programming and robots, it is also a powerful tool in understanding complex robot behaviour, which we have now learned the foundation for. 
    We have also learned that in order to program a robot, it is crucial to be structured and work systematically with the different aspects this encompasses. The \emph{Robot operating system} provides this structure, which
    again gives the foundation for later working with more complex robots.
    Finally we have also found that mathematical analysis is a powerful tool in optimizing the robot behaviour. 
    This is especially important in the case of obstacle avoidance, which is the main reason for the existence of this project.
    


    \section{Personal contributions}
    In programming the Arduino, it was mainly Andreas who did the coding and implementation on the breadboard. In this part of the course, Asger spent a great time on documenting the progress we made, which came in very useful when composing this final report. 
    We set up ROS on Andreas' Windows computer, which meant it was mainly Andreas who did the concrete coding. We did however collaborate closely throughout the entire tutorial. 

    
    
    For programming and SSH'ing into the Turtlebot we have exclusively used Andreas' PC. We chose this since we encountered some difficulties in working on Asger's MacBook. 
    We did however collaborate closely throughout the entire project. 
    In our final code, Andreas wrote the functions \texttt{small\_turns, make\_180} and \texttt{non\_zeros}. 
    Asger wrote the functions \texttt{emergency\_check} and \texttt{get\_scan}, in which he also got the idea for the blind angles. 
    For the rest of the code we wrote it in close dialogue and discussed all implementations carefully. The depicted power regressions and similar GeoGebra drawings were made by Andreas. 
    The drawings of cases and matching actions were made by Asger. 
    
    For this report, Asger wrote the following sections:
    \begin{itemize}
        \item Abstract
        \item Part IV-C (subsection 1, 2, and 3)
        \item Part IV-D (subsection 5)
        \item Part V-C (subsection 2)
        \item Part V-D (subsection 5)
    \end{itemize}

    Andreas wrote:
    \begin{itemize}
        \item Part I
        \item Part II
        \item Part III
        \item Part IV-A
        \item Part IV-B
        \item Part IV-C (subsection 4 and 5)
        \item Part IV-D (subsection 1, 2, 3, 4)
        \item Part V-A
        \item Part V-B
        \item Part V-C (subsection 1, 2, 3)
        \item Part V-D (subsection 1, 2, 3, 4)
    \end{itemize}    
    
    We did also lose a member of our group in the middle of the project, which meant we were only 2 left.
    For this reason it did not make much sense to divide the different parts of the project between us. 
    As a result we have both been focusing on getting a deep understanding of all aspects of the project and associated learning outcomes of the course. 


    \begin{thebibliography}{00}\label{sec:References}
        \bibitem{b1} Course material from Brightspace. 
        \bibitem{b2} \href{http://wiki.ros.org/ROS/Tutorials}{ROS beginner's tutorial.}
        \bibitem{b3} \href{https://let-elektronik.dk/shop/300-arduino-boards/12640-pro-micro---5v16mhz/}{Arduino PRO micro Specifications.}
        \bibitem{b4} \href{https://let-elektronik.dk/shop/1440-afstand--bevaegelse/8502-ultrasonic-range-finder---lv-maxsonar-ez0/}{Data sheet and specifications for Ultrasonic Range Finder.}
        \bibitem{b5} \href{https://let-elektronik.dk/shop/1450-lys--kamera/12829--rgb-light-sensor---isl29125/}{Data sheet and specifications for RGB light sensor.}
        \bibitem{b6} \href{https://github.com/ROBOTIS-GIT/turtlebot3/blob/master/turtlebot3\_example/nodes/turtlebot3\_obstacle}{First robot navigation example from GitHub.}
        \bibitem{b7} \href{https://emanual.robotis.com/docs/en/platform/turtlebot3/features/}{Turtlebot3 specifications.}
        \bibitem{b8} \href{https://www.robotis.us/360-laser-distance-sensor-lds-01-lidar/}{LiDAR sensor specifications. }
        \bibitem{b9} \href{https://github.com/Andreas691667/Computer_project_I}{Own GitHub repository containing Arduino code and final Turtlebot code.}
    \end{thebibliography}

\end{document}