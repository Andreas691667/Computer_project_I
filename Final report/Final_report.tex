\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{img/}}
\usepackage{float}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}

% pakker til c kode
\usepackage{listings}
\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{my_python}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=2pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1,
    language=Python
    % basicstyle = \tiny
}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pagestyle{fancy}
\fancyhf{}
\rhead{\today}
\chead{Computer Project I -- Final Report}
\lhead{A. H. S. Poulsen \& A. K. Thomsen}
\rfoot{Page \thepage}

\begin{document}

\title{Computer Project I -- Final Report} 

\author{\IEEEauthorblockN{1\textsuperscript{st} Asger Høøck Song Poulsen }
\IEEEauthorblockA{\textit{Undergraduates at Aarhus University,} \\
\textit{Dept. of electrical and computer engineering}\\
Study number: 202106630\\
AU-id: au704738}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Andreas Kaag Thomsen}
\IEEEauthorblockA{\textit{Undergraduates at Aarhus University,} \\
\textit{Dept. of electrical and computer engineering}\\
Study number: 202105844 \\
AU-id: au691667}
}

\maketitle

\begin{abstract}
    This is our abstract.
\end{abstract}

\section{Introduction}
    Overall, the project can be divided into four parts with certain aim and objectives: 
    \begin{enumerate}
        \item Understanding the basic concepts of robotics programming by working with an Arduino - that is, the \emph{See-think-act cycle} (see figure \ref{fig:See-think-act}). 
        Reading and analyzing data from a range sensor and RGB sensor.
        \item ROS Programming on Raspberry PI. Learning the structure that ROS provides and going through the beginner's tutorial. 
        \item Programming the robot to avoid obstacles in different scenarios. 
        \item Optimizing the robot's performance both with respect to linear speed and collision avoidance. Finalizing the code and testing the robot on an obstacle course. 
    \end{enumerate}

\section{Specifications}
We have used the following equipment throughout the course. 
\begin{itemize}
    \item Arduino PRO MICRO - 5V/16MHZ 
    \item Ultrasonic Range Finder (LV-MAXSONAR-EZ0)
    \item RGB Light Sensor ISL29125
    \item LED's, cables etc.
    \item Turtlebot3 Burger Robot equipped with i.a. a Raspberry Pi 3 and a 360\textdegree LiDAR sensor.  
\end{itemize}
For coding we have used the language C for programming the Arduino on the Arduino software. 
For programming the Turtlebot we have been using Python and the command prompt with the built in nano-editor. 

\section{Design and implementation}
    In this section we are going to describe our thoughts on design and implementation of the project.
    We are only going to desribe the underlying abstract thoughts and the concrete implementation will be elaborated in section \ref{sec:experiment_setup_and_result}.

    \subsection{Part 1: Arduino Programming}
    We were to consider the \emph{See-think-act} cycle as can be seen in figure \ref{fig:See-think-act} below:
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{see_think_act.png}
        \caption{See-think-act cycle}
        \label{fig:See-think-act}
    \end{figure}
    On our breadboard we connected the Arduino to which we connected the ultrasonic range finders and som LED's. 
    The cycle was then implemented as follows: the range finder sensors \textbf{saw} any obstacle we put in front of it, and sent some data back to Arduino. 
    This data was computed - that is the \textbf{think} step of the cycle. Then the \textbf{act} step was performed, which
    in this case was the LED blinking. \\
    Afterwards we extended the circuit to include three range finder sensors plus the RGB light sensor and four LED's. 
    This thus acted as a simulation of the real Turtlebot, which we should work with in part 3. 

    \subsection{Part 2: ROS Programming}
    In this part of the course we worked on a Virtual Machine (VM) on which we had installed Ubuntu. 
    We went through the ROS beginner's tutorial and learned the structure of a ROS system. 

    \subsection{Part 3: Programming the robot to avoid obstacles}
    As described, the Turtlebot is equipped with a LiDAR 360\textdegree laser sensor, which measures the distance.
    It continuosly returns an array: 
    \begin{displaymath}
        \texttt{dist} = [d_0, d_1, \dots, d_{359}]
    \end{displaymath}
    where \(d_i\) is the distance to the nearest object at angle \(i\). 
    We decided to look at a span of 120 degrees, which we divided into three distinct parts:
    \begin{align*}
        \texttt{left}  &= [d_{15}, d_{16},\dots,d_{60}], \texttt{N=45} \\
        \texttt{front} &= [d_{0},d_{1},\dots,d_{14}, d_{345}, d_{346},\dots, d_{359}], \texttt{N=30} \\
        \texttt{right} &= [d_{300}, d_{301},\dots, d_{344}], \texttt{N=45}
    \end{align*}
    We decided to do this so the robot more often would \emph{turn} instead of just driving \emph{backwards}, 
    since this supposedly would achieve an overall higher linear speed. \par
    For the different cases of obstacles we have made several cases of \texttt{if/elif} statements. 
    The cases and the decisions in each case can be seen in figure \ref{fig:obstacle_cases} below.
    \begin{figure}[H]
        \centering
            \includegraphics*[scale = 0.5]{obstacle_cases.png}   
        \caption{Obstacle cases}
        \label{fig:obstacle_cases}
    \end{figure}

    \subsection{Part 4: Optimizing the robot's performance }
    In the last part of the course we should test the robot on an arbitrary course of obstacles.
    We did, however, test the robot continuosly in part 3, so we focused this part on optimizing the existing code. The goal for the optimization part of this course was to maintain the linear speed as high as possible and the number of collisions as low as possible.
    This will be elaborated in subsection \ref{subsec:partIV_4}.   
     

\section{Experiment setup and results}\label{sec:experiment_setup_and_result}

    \subsection{Part 1: Arduino Programming}
    We connected the Range sensor to the Arduino and intialized it as follows:
    \begin{lstlisting}[style = my_python, caption = Initialization of variables]        
int SENSOR = A0; //range sensor connected to port A0
double range_input = 0; //variable for storing input values
void setup()
{
  pinMode(SENSOR,INPUT); //sensor declarared as an input
} 
\end{lstlisting}
In the control loop we updated \texttt{range\_input} with the value read from the sensor with using the \texttt{analogRead} function.
Through a series of \texttt{if-else} statements we made the LED blink with different intervals for different distances:
\begin{lstlisting}[style = my_python, language = C, caption = \texttt{else-if} statements for Arduino, basicstyle = \footnotesize]
if (range_input+margin < 20 && range_input+margin > 0) {
    TXLED0;
    delay(50);
    TXLED1;
    delay(50);
}

else if (range_input+margin < 30 && range_input+margin > 25) {
    TXLED0;
    delay(333);
    TXLED1;
    delay(333);
} 
    
else if(range_input+margin < 25 && range_input+margin >20) {
    TXLED0;
    delay(1000);
    TXLED1;
    delay(1000);
}
TXLED1; //LED turned off by default  
\end{lstlisting}

Afterwards we extended the circuit to include three range sensors in the same way as above. 
We discovered that our measurements at first were wrong since we did not convert the input values to distances. 
Looking at the equipment specifications we found out that the sensor uses a scaling factor of \(6.4 \frac{mV}{inch}\). 
By dividing the analog input with this scaling factor, a measurement in inches is calculated. Afterwards, this is multiplied by \(2.54 \frac{cm}{inch}\).
\begin{lstlisting}[style = my_python, caption = Calculations of ranges, basicstyle = \footnotesize]
front_input_cm = (front_input/6.4)*2.54;
left_input_cm = (left_input/6.4)*2.54;
right_input_cm = (right_input/6.4)*2.54;
\end{lstlisting}
Now we had the correct measurements which we also manually confirmed with a ruler. \\
Lastly we also implemented the RGB light sensor on the breadboard, which was intialized in the same way as the range sensors. 
The RGB sensor returned one value measured in lux for the luminance. If the luminance was less than 30,
one LED was turned on. With everything connected as described above, our final circuit is depicted in figure \ref{fig:arduino_final} below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth, angle = 90]{arduino_final.jpeg}
    \caption{Final Arduino circuit}
    \label{fig:arduino_final}
\end{figure}

    \subsection{Part 2: ROS Programming}
    As mentioned we followed the ROS beginner's tutorial in which we installed ROS using several commands. 
    We got stuck in part 2 of the tutorial, so we decided to go back to the beginning and reinstall everything.
    After doing this, we were able to complete all the steps from 1 through 17. 
    We got a deeper understanding of how the ROS system is structured and learned several ROS-related commands. 
    We will explain the most essential for our further work with the Turtlebot below\footnote{These definitions are taken from the ROS beginner's tutorial, which is linked in section \ref{sec:References}}:
    \begin{enumerate}
        \item \textbf{roscore}: roscore is a collection of nodes and programs that are pre-requisites of a ROS-based system. You must have a roscore running in order for ROS nodes to communicate.
        \item \textbf{rosrun}: rosrun allows you to run an executable in an arbitrary package from anywhere without having to give its full path. 
        \item \textbf{source}: When opening a new terminal your environment is reset and you are obliged to use the \texttt{source} command to re-source the .bashrc file. 
        \item \textbf{nano}: nano is a text editor that is used to edit files. We used it to edit our code files directly in the terminal.
    \end{enumerate}

 
    \subsection{Part 3: Programming the robot to avoid obstacles}
    In this part we are going to describe our initial implementation. The optimization we did, will be described in part 4. \\
    We were given a robot navigation example from a GitHub repository from where we were to get inspiration and further implement our own robot navigation. The idea of the initial example was to: \emph{Drive forward until we get close to an obstacle, then stop}. \\
    For case \(1\) we initially programmed the robot to back out of the blind alley for  and then turn around:
\begin{lstlisting}[style = my_python, caption = Case 1, basicstyle = \footnotesize]
if (right_distance < SAFE_STOP_DISTANCE and left_distance < SAFE_STOP_DISTANCE and front_distance < SAFE_STOP_DISTANCE):
    if turtlebot_moving:
        twist.linear.x = -LINEAR_VEL
        twist.angular.z = 0.0
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning backwards for 1 second')
        t.sleep(1.0)
        twist.linear.x = 0.0
        twist.angular.z = 3.14/2
        self._cmd_pub.publish(twist)
        rospy.loginfo('Turning around')
        t.sleep(1.0)
        turtlebot_moving = False
\end{lstlisting}
    For case \(2\) the robot was programmed to look to the left and right respectively and then turn in the opposite direction of the closest obstacle:
\begin{lstlisting}[style = my_python, caption = Case 2, basicstyle = \footnotesize]
elif (front_distance < SAFE_STOP_DISTANCE + 0.05):
    if turtlebot_moving:
        if (right_distance < left_distance):
            small_turns('left', turn_angle + 0.2)
        elif(left_distance < right_distance):
            small_turns('right', turn_angle + 0.2)
\end{lstlisting}
    Lastly, for case \(3\) and \(4\): If an obstacle on the left is within the \texttt{SAFE\_STOP\_DISTANCE} turn right and vice versa.
\begin{lstlisting}[style = my_python, caption = Case 3 \& 4, basicstyle = \footnotesize]
elif(left_distance < SAFE_STOP_DISTANCE or right_distance < SAFE_STOP_DISTANCE):
    if turtlebot_moving:
        if(right_distance < left_distance):
            small_turns('left', turn_angle)
        else:
            small_turns('right', turn_angle)
\end{lstlisting}


%%%%% NON ZEROS FUNKTION TIL DIG ASGER <3
\begin{lstlisting}[style = my_python, caption = Non-zero function, basicstyle = \footnotesize, label = lst:non_zeros]
# function converts list of tuples to list of lists and then converts zeros to 3.5:
def non_zeros(l):
    # convert tuples to list
    for i in range(len(l)):
        l[i] = list(l[i])

    # if element zero, set to 3.5
    for i in range(len(l)):
        for j in range(len(l[i])):
            if l[i][j] == 0:
                l[i][j] = 3.5
\end{lstlisting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \subsection{Part 4: Optimizing the robot's performance }\label{subsec:partIV_4}
    In this part we are going to describe the changes we applied to the code setup from part 3 and the results we got. \\
    \subsubsection{Adding a \texttt{factor} to \texttt{SAFE\_STOP\_DISTANCE}}
    We experienced that when the robot encountered an obstacle on its front it did not manage to avoid it optimally. 
    We thus added a factor of \(0.08\)cm to the \texttt{SAFE\_STOP\_DISTANCE} constant in this case. By doing this, the robot began avoiding the obstacle earlier, 
    and thus it was able to avoid it more optimally. \\
    
    \subsubsection{Making sure to avoid the obstacle before next control loop iteration}\label{subsec:avoid_obstacle}
    We wanted to make sure that the robot actually avoided the obstacle before the next control loop iteration.
    We did this by adding a \texttt{while}-loop to each case as shown in listing \ref{lst:while_loops}.
\begin{lstlisting}[style = my_python, caption = \texttt{while}-loops for obstacle avoidance, basicstyle = \footnotesize, label=lst:while_loops]
# case 1: obstacle in front
if (front_distance < SAFE_STOP_DISTANCE + 0.08):
    print('obstacle in front')
    # look ahead:
    if turtlebot_moving:
        if (right_distance < left_distance):
            while(front_distance < SAFE_STOP_DISTANCE + factor):
                small_turns('left', 'angular vel', 'linear vel.')
                front_new = self.get_scan()
                non_zeros(front_new)
                emergency_check(front_new)
                front_distance = np.mean(front_new[1] + front_new[2]) \end{lstlisting}
    By doing this we ensured that the robot would not stop avoiding the obstacle before it had actually escaped it\footnote{Note that the 2nd and 3rd argument in the \texttt{small\_turns} function are written in pseudo-code since we did further optimizations, which will be described in subsection \ref{subsec:optimize_linear_speed}. 
    Same goes for the variable \texttt{factor}, which was described in the subsection before. The function \texttt{emergency\_check} will also be described in subsection \ref{subsec:emergency_check}.}.
    For each loop iteration we made a new laser scan and checked if the robot was within the \texttt{SAFE\_STOP\_DISTANCE+factor}. If it is, we called the \texttt{small\_turns} function,
    and turned the robot in the opposite direction of the closest obstacle as normally.     

    \subsubsection{Checking the blind angles}
    In addition to the three directions we had already implemented, we decided to focus on two more directions; the so-called blind angles.
    These were meant to make the robot able to avoid an obstacle if it was only slightly in front of it, but not so much that the average would get low enough to cause a turn event. 
    We calculated these by measuring the minimal distance between two obstacles that robot could drive through - that is, the width of the robot, which we measured to be \(20\)cm.
    We wanted the robot to begin avoiding the obstacles at the blind angles from a distance of \(40\)cm. We got the results using GeoGebra as depicted in figure \ref{fig:blind_angle}. 
    We also wanted these angles to be very narrow such that the robot would be able to detect even a small corner of some obstacle. For this reason we only looked at a span of \(8\) degrees. 
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{blind_angle.png}
    \caption{Blind angles}
    \label{fig:blind_angle}
    \end{figure}
    We thus included the following code in our function \texttt{get\_scan()}:
    \begin{lstlisting}[style = my_python, caption = Blind angles in \texttt{get\_scan()}, basicstyle = \footnotesize]
front_left_lidar_samples = scan.ranges[14:22]
front_right_lidar_samples = scan.ranges[338:346]

scan_filter.append(front_left_lidar_samples)
scan_filter.append(front_right_lidar_samples) \end{lstlisting}
    In addition to this, we added an extra case (named case 3) for checking these blind angles. This was implemented in the exact same way as desribed in subsection \ref{subsec:avoid_obstacle} using another \texttt{elif}-statement. 
    The robot should detect these blind angles quite early, so we added a factor of \(0.15\)cm to the \texttt{SAFE\_STOP\_DISTANCE} constant. \\

    \subsubsection{Optimizing linear speed}\label{subsec:optimize_linear_speed}
    For our initial implementation, our robot would \emph{turn} with a constant linear speed of \(0.1 \frac{m}{s}\). Our goal was to achieve an overall higher linear speed.
    We did this by considering the relationship between the linear speed and distance to object as well as the relationship between the angular speed and distance to object.
    We wanted the following:
    \begin{itemize}
        \item The linear speed should be \textbf{smaller}, the closer the obstacle was.
        \item The angular speed should be \textbf{greater}, the closer the obstacle was.
    \end{itemize}
    To do this, we considered some distinct cases\footnote{These speeds were tested empirically until we found appropriate values.}:
    \begin{enumerate}
        \item Robot is at maximal \texttt{SAFE\_STOP\_DISTANCE} away from the obstacle, that is \(0.4\)m for blind angle obstacles and \(0.33\)m for front obstacles.
        \begin{itemize}
            \item Linear speed should be \texttt{MAX\_LINEAR\_VEL}, that is \(0.22 \frac{m}{s}\).
            \item Angular speed should be minimal, that is \(0.79 \frac{rad}{s}\) for blind angle obstacles and \(2.0 \frac{rad}{s}\) for front obstacles. 
        \end{itemize}
        \item Robot is crititacally close to the obstacle, that is \(0.1\)m. 
        \begin{itemize}
            \item Linear speed should essentially be zero, that is \(0.01 \frac{m}{s}\).
            \item Angular speed should be \texttt{MAX\_ANGLE}, that is \(2.84 \frac{rad}{s}\). 
        \end{itemize}
    \end{enumerate}
    With these values, we made power regression using GeoGebra. The result for linear velocity is depicted in figure \ref{fig:lin_vel_graph}. Here, \(p(x)\) is the linear velocity in \(\frac{m}{s}\), and \(x\) is the distance to the obstacle in m.
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.3\textwidth]{lin_vel_graph.png}
            \caption{Linear velocity graph}
            \label{fig:lin_vel_graph}
        \end{figure}
    The result for angular velocity is depicted in figure \ref{fig:ang_vel_graph}. Here, \(g(x)\) and \(h(x)\) are the angular velocities in \(\frac{rad}{s}\) for front obstacles and blind spot obstacles, respectively.
    \(x\) is the distance to the obstacle in m.        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.4\textwidth]{ang_vel_graps.png}
            \caption{Angular velocity graphs}
            \label{fig:ang_vel_graph}
        \end{figure}
    Afterwards, we implemented this in our code. We needed to include the power functions we had found each time we called the \texttt{small\_turns} function. 
    An example for the blind spot case is shown in listing \ref{lst:blind_spot}.
\begin{lstlisting}[style = my_python, caption = Power functions implemented for blind spot, basicstyle = \footnotesize, label = lst:blind_spot]
small_turns('right', 0.34*front_left_distance**
(-0.93), 1.7*front_left_distance**2.23)
\end{lstlisting}
    After implementing these changes in the code, we significantly improved the robot's linear speed. \\

    \subsubsection{Better collision avoidance}\label{subsec:emergency_check}
    Another goal of our optimization was to avoid collisions with obstacles completely. We did this by implementing a new function called \texttt{emergency\_check} that would check if the robot was crititacally close to an obstacle. 
    We defined the robot to be critically close to an obstacle if it is within \(0.1\)m of an obstacle. With a \texttt{LIDAR\_ERROR} of \(0.05\)m, we introduced a new global variable \texttt{EMERGENCY\_STOP\_DISTANCE = 0.15}. 
    If it was, the robot's linear speed would be set to zero and the robot would turn with max angular speed. The function is depicted in listing \ref{lst:emergency_check}.
    \begin{lstlisting}[style = my_python, caption = Emergency check, basicstyle = \footnotesize, label = lst:emergency_check]
def emergency_check(l):
    # calculate mean of our slices:
    left_distance = np.mean((l[0]))
    front_distance = np.mean(l[1] + l[2])
    right_distance = np.mean(l[3])
    front_left_distance = np.mean(l[4])
    front_right_distance = np.mean(l[5])

    # calculate mean of special case slices
    special_case_right = np.mean(l[3][27:35])
    special_case_left = np.mean(l[0][10:18])

    if (front_distance < EMERGENCY_STOP_DISTANCE and left_distance < EMERGENCY_STOP_DISTANCE):
        small_turns('right', MAX_ANGLE, 0.0)

    elif (front_distance < EMERGENCY_STOP_DISTANCE and right_distance < EMERGENCY_STOP_DISTANCE):
        small_turns('left', MAX_ANGLE, 0.0)

    elif (front_distance < EMERGENCY_STOP_DISTANCE):
        if (left_distance < right_distance):
            small_turns('right', MAX_ANGLE, 0.0)
        else:
            small_turns('left', MAX_ANGLE, 0.0)

    elif(special_case_right < SAFE_STOP_DISTANCE and special_case_left < SAFE_STOP_DISTANCE):
        make_180()

    elif(front_left_distance < SAFE_STOP_DISTANCE):
        small_turns('right', MAX_ANGLE, 0.0)

    elif(front_right_distance < SAFE_STOP_DISTANCE):
        small_turns('left', MAX_ANGLE, 0.0) \end{lstlisting}

    The function takes an input \texttt{l} which is a list of tuples containing the latest readings. 
    We called this function for each loop iteration, both in the overall control loop as well as in the loops described in subsection \ref{subsec:avoid_obstacle} and depicted in listing \ref{lst:while_loops}.
    By doing this we constantly checked if the robot was so close to an obstacle that it could not avoid it by turning conventionally. 
    Instead, the robot should turn \emph{in place} with maximal angular speed.     

\section{Discussion}
In this section of the report we will discuss some of the difficulties we encountered during the project.
We are also going to discuss whether or not we reached our goal.
Again, we have divided it into the four main parts of the project described earlier. 

    \subsection{Part 1: Arduino Programming}
    \begin{enumerate}
        \item \textbf{Computing the readings correctly} \\
        We experienced some difficulties in computing the readings correctly since we were not able to find the right scaling factor. 
        We found that many sensors were to be treated differently since they did not give consistent readings. However, since this was only a
        preliminary exercise to the main project, we did not give it that much thought. 
        \item \textbf{Preparation to main project} \\
        Working with the Arduino and the appurtenant sensors helped us understand the importance of the see-think-act cycle as shown in figure \ref{fig:See-think-act}.
        Our code structure turned out to be predominantly alike in the way we worked with the \texttt{if/elif}-statements. 
        This was of course only with respect to some very simple cases, but the underlying though processes were not very different. 
        The body of the \texttt{if/elif}-statements was also way more complex in our robot's code. 
        Moreover, we found that working with the sensors was not that easy and that it demanded a lot of effort and reading to actually understand the input from the sensors.
        This was a valuable lesson that prompted us to work more structured in the main project.  
    \end{enumerate}


    \subsection{Part 2: ROS Programming}
    \begin{enumerate}
        \item \textbf{Understanding the ROS structure} \\
    \end{enumerate}


    \subsection{Part 3: Programming the robot to avoid obstacles}
    \begin{enumerate}
        \item \textbf{Getting wrong sensor measurements} \\
        We experienced quite a difficulty in getting the right measurements from the sensor.
        We knew that it would return an array of size 360 with one distance for each angle. 
        However, we couldn't figure out how they were indexed and our robot thus behaved incorrectly and sometimes
        turned left when it should turn right.
        We solved the problem by manually printing out different test angles and then physically measure that angle also. 
        For instance printing \texttt{scan\_ranges[45]} which gave some change in output distance for some specific angle, which we measured. 
        In turned out that it our code should be reversed. 
        \item \textbf{Using \texttt{mean} instead of \texttt{min} and resolving problems with faulty readings} \\
        Our initial code setup from GitHub used Python's \texttt{min} function to find the representative for each direction's minimum distance.
        However, we found that our readings from the LiDAR contained a lot of zeros, which caused this representative to be zero very often. 
        For this reason we decided to calculate the average of the readings instead using Numpy's \texttt{mean} function.
        After doing this we experienced that our readings became much more representative of the actual distance.\\
        Moreover, since our readings still contained a lot of zeros, we modified our code to use the \texttt{non\_zeros}-function as shown in listing \ref{lst:non_zeros}.
        This converted potential zeros to the sensor's max range of \(3.5\)m. 
        This could however actually become a problem since both very close obstacles and very far obstacles could be represented by the same distance of 0.
        Thus our average value could become quite ambiguous. A better way to deal with this problem could be to \emph{remove} these faulty reading, making the average more precise and more reliable.
        \item \textbf{Finding the right span} \\
        As mentioned, we had decided to look at a span of \(120\) degrees, which is not coincidental. 
        \begin{figure}
            \centering
            \includegraphics[width=0.45\textwidth]{120_degrees_argumentation.png}
            \caption{The span of the robot's sensor.}
            \label{fig:span}
        \end{figure}

    \end{enumerate}


    \subsection{Part 4: Optimizing the robot's performance }
    \begin{enumerate}
        \item \textbf{Structuring the \texttt{else-if} statements} \\
        \item \textbf{Finding the proper regression type} \\
    \end{enumerate}

% \section{Personal contributions}

\section{References}\label{sec:References}
    \begin{itemize}
        \item Course material from Brightspace.   
        \item ROS beginner's tutorial: \emph{http://wiki.ros.org/ROS/Tutorials}
        \item Data sheet and specifications for Ultrasonic Range Finder: \emph{shorturl.at/cBHN1}
        \item Data sheet and specifications for RGB light sensor: \\ \emph{shorturl.at/htEP8}
        \item First robot navigation example: \emph{shorturl.at/lpuM8}
    \end{itemize}



\end{document}